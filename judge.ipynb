{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Load models, Create evaluation matrix\n",
    "models_map = {\n",
    "    # \"lora_llama\": \"EITD/lora_model_1\",\n",
    "    \"orpo_llama\": \"EITD/orpo_llama\",\n",
    "    \"origin_llama\": \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "}\n",
    "\n",
    "tokenizers, models, evaluation_datasets = {}, {}, {}\n",
    "\n",
    "for model_name, model_path in models_map.items():\n",
    "    tokenizers[model_name] = AutoTokenizer.from_pretrained(model_path)\n",
    "    models[model_name] = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    evaluation_datasets[model_name] = Dataset.from_dict({\"index\": [],\"response\": [], \"feedback\": [],\"generate time\": [],\"score\": []})\n",
    "\n",
    "# Load dataset for evaluation\n",
    "# dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset_name = \"mlabonne/FineTome-100k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\").shuffle(seed=42)\n",
    "# messages = [{\"role\": \"user\", \"content\": prompt} for prompt in dataset[\"test\"][\"prompt\"]]\n",
    "# messages = [{\"role\": \"user\", \"content\": feature['conversations'][0]['value']} for feature in dataset['test']]\n",
    "messages = dataset.map(\n",
    "    lambda row: {\"role\": \"user\", \"content\": next((item[\"value\"] for item in row[\"conversations\"] if item[\"from\"] == \"human\"), None)},\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "messages = messages.select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResponse(model_name, messages):\n",
    "    tokenizer = tokenizers.get(model_name)\n",
    "    model = models.get(model_name)\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name) # can use dataSet here\n",
    "\n",
    "    for i, message in enumerate(dataset['test']):\n",
    "        \n",
    "        start = time.time()         # start time\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [message],\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        )\n",
    "        \n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                                temperature = 1.5, min_p = 0.1)\n",
    "        \n",
    "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        end = time.time()           # end time\n",
    "\n",
    "        if \"assistant\" in response:\n",
    "            response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        evaluation_dataset = evaluation_dataset.add_item({\"index\": i, \"response\": response, \"feedback\": None, \"generate time\": end-start, \"score\": None})\n",
    "    \n",
    "    evaluation_datasets[model_name] = evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample data\n",
    "from flow_judge import Llamafile, EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "\n",
    "# Initialize the model\n",
    "judge = Llamafile()\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=judge\n",
    ")\n",
    "\n",
    "def add_feedback_and_score(example, result):\n",
    "    return {\n",
    "        \"response\": example['response'],\n",
    "        \"generate time\": example['generate time'],\n",
    "        \"feedback\": result.feedback,\n",
    "        \"score\": result.score\n",
    "    }\n",
    "\n",
    "def saveJudge(model_name, messages):\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "\n",
    "    # Create a list of inputs and outputs\n",
    "    inputs_batch = [\n",
    "        [\n",
    "            {\"query\": message[\"content\"]},\n",
    "            {\"context\": \"\"},\n",
    "        ]\n",
    "        for message in messages\n",
    "    ]\n",
    "    outputs_batch = [{\"response\": item[\"response\"]} for item in evaluation_dataset]\n",
    "\n",
    "    # Create a list of EvalInput\n",
    "    eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "    # Run the batch evaluation\n",
    "    results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)\n",
    "\n",
    "    # Visualizing the results\n",
    "    evaluation_datasets[model_name] = evaluation_dataset.map(lambda item: add_feedback_and_score(item, results[item['index']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_map.keys():\n",
    "    saveResponse(model_name, messages)\n",
    "    saveJudge(model_name, messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
