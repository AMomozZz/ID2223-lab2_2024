{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orpo_llama:\n",
      "origin_llama:\n",
      "mlabonne/FineTome-100k:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# Load models, Create evaluation matrix\n",
    "models_map = {\n",
    "    # \"lora_llama\": \"EITD/lora_model_1\",\n",
    "    \"orpo_llama\": \"EITD/orpo_llama\",\n",
    "    \"origin_llama\": \"unsloth/Llama-3.2-1B-Instruct\",\n",
    "}\n",
    "\n",
    "tokenizers, models, evaluation_datasets = {}, {}, {}\n",
    "\n",
    "for model_name, model_path in models_map.items():\n",
    "    print(model_name + \":\")\n",
    "    tokenizers[model_name] = AutoTokenizer.from_pretrained(model_path)\n",
    "    models[model_name] = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    evaluation_datasets[model_name] = Dataset.from_dict({\"index\": [],\"response\": [], \"feedback\": [],\"generate time\": [],\"score\": []})\n",
    "\n",
    "# Load dataset for evaluation\n",
    "# dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset_name = \"mlabonne/FineTome-100k\"\n",
    "print(dataset_name + \":\")\n",
    "dataset = load_dataset(dataset_name, split=\"all\").shuffle(seed=42)\n",
    "# messages = [{\"role\": \"user\", \"content\": prompt} for prompt in dataset[\"test\"][\"prompt\"]]\n",
    "# messages = [{\"role\": \"user\", \"content\": feature['conversations'][0]['value']} for feature in dataset['test']]\n",
    "messages = dataset.map(\n",
    "    lambda row: {\"role\": \"user\", \"content\": next((item[\"value\"] for item in row[\"conversations\"] if item[\"from\"] == \"human\"), None)},\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "messages = messages.select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResponse(model_name, messages):\n",
    "    tokenizer = tokenizers.get(model_name)\n",
    "    model = models.get(model_name)\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name) # can use dataSet here\n",
    "\n",
    "    for i, message in enumerate(messages):\n",
    "        \n",
    "        start = time.time()         # start time\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [message],\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        )\n",
    "        \n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                                temperature = 1.5, min_p = 0.1)\n",
    "        \n",
    "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        end = time.time()           # end time\n",
    "\n",
    "        if \"assistant\" in response:\n",
    "            response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        evaluation_dataset = evaluation_dataset.add_item({\"index\": i, \"response\": response, \"feedback\": None, \"generate time\": end-start, \"score\": None})\n",
    "    \n",
    "    evaluation_datasets[model_name] = evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orpo_llama: responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin_llama: responses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for model_name in models_map.keys():\n",
    "    print(model_name + \": responses\")\n",
    "    saveResponse(model_name, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orpo_llama:\n",
      "None 32.386916399002075 None\n",
      "None 33.40937304496765 None\n",
      "None 29.25759267807007 None\n",
      "origin_llama:\n",
      "None 25.634678840637207 None\n",
      "None 29.464585781097412 None\n",
      "None 26.37605357170105 None\n"
     ]
    }
   ],
   "source": [
    "for model_name in models_map.keys():\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "    print(model_name + \":\")\n",
    "    for i in range(len(evaluation_dataset)):\n",
    "        print(evaluation_dataset[i][\"score\"], evaluation_dataset[i][\"generate time\"], evaluation_dataset[i][\"feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-04 04:30:13 _custom_ops.py:18] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\n"
     ]
    }
   ],
   "source": [
    "# Read the sample data\n",
    "from flow_judge import Llamafile, EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "\n",
    "# Initialize the model\n",
    "judge = Llamafile(\n",
    "    gpu_layers=0,\n",
    "    flash_attn=False,\n",
    "    quantized_kv=False,\n",
    ")\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=judge\n",
    ")\n",
    "\n",
    "def add_feedback_and_score(example, result):\n",
    "    return {\n",
    "        \"response\": example['response'],\n",
    "        \"generate time\": example['generate time'],\n",
    "        \"feedback\": result.feedback,\n",
    "        \"score\": result.score\n",
    "    }\n",
    "\n",
    "def saveJudge(model_name, messages):\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "\n",
    "    # Create a list of inputs and outputs\n",
    "    inputs_batch = [\n",
    "        [\n",
    "            {\"query\": message[\"content\"]},\n",
    "            {\"context\": \"\"},\n",
    "        ]\n",
    "        for message in messages\n",
    "    ]\n",
    "    outputs_batch = [{\"response\": item[\"response\"]} for item in evaluation_dataset]\n",
    "\n",
    "    # Create a list of EvalInput\n",
    "    eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "    # Run the batch evaluation\n",
    "    results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)\n",
    "\n",
    "    # Visualizing the results\n",
    "    evaluation_datasets[model_name] = evaluation_dataset.map(lambda item: add_feedback_and_score(item, results[item['index']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orpo_llama: feedbacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting llamafile server...\n",
      "INFO:root:Llamafile path: C:\\Users\\silve\\.cache\\flow-judge\\flow-judge.llamafile\n",
      "INFO:root:Starting llamafile server with command: sh -c 'C:\\Users\\silve\\.cache\\flow-judge\\flow-judge.llamafile --server --host 127.0.0.1 --port 8085 -c 8192 -ngl 0 --temp 0.1 -n 1000 --threads 32 --nobrowser -b 32 --parallel 1 --cont-batching'\n",
      "ERROR:root:Error starting llamafile server: [WinError 2] 系统找不到指定的文件。\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py\", line 369, in start_llamafile_server\n",
      "    self.llamafile_process = self._start_llamafile_process(command)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py\", line 422, in _start_llamafile_process\n",
      "    process = subprocess.Popen(\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python3112\\Lib\\subprocess.py\", line 1024, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Python3112\\Lib\\subprocess.py\", line 1493, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [WinError 2] 系统找不到指定的文件。\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m models_map\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: feedbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     \u001b[43msaveJudge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36msaveJudge\u001b[1;34m(model_name, messages)\u001b[0m\n\u001b[0;32m     40\u001b[0m eval_inputs_batch \u001b[38;5;241m=\u001b[39m [EvalInput(inputs\u001b[38;5;241m=\u001b[39minputs, output\u001b[38;5;241m=\u001b[39moutput) \u001b[38;5;28;01mfor\u001b[39;00m inputs, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs_batch, outputs_batch)]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Run the batch evaluation\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfaithfulness_judge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_inputs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Visualizing the results\u001b[39;00m\n\u001b[0;32m     46\u001b[0m evaluation_datasets[model_name] \u001b[38;5;241m=\u001b[39m evaluation_dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m item: add_feedback_and_score(item, results[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\flow_judge\\flow_judge.py:104\u001b[0m, in \u001b[0;36mFlowJudge.batch_evaluate\u001b[1;34m(self, eval_inputs, use_tqdm, save_results, fail_on_parse_error)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(eval_inputs)\n\u001b[0;32m    103\u001b[0m prompts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt(eval_input) \u001b[38;5;28;01mfor\u001b[39;00m eval_input \u001b[38;5;129;01min\u001b[39;00m eval_inputs]\n\u001b[1;32m--> 104\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m eval_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    106\u001b[0m     EvalOutput\u001b[38;5;241m.\u001b[39mparse(response, fail_on_parse_error\u001b[38;5;241m=\u001b[39mfail_on_parse_error)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m responses\n\u001b[0;32m    108\u001b[0m ]\n\u001b[0;32m    109\u001b[0m parse_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m eval_outputs \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py:527\u001b[0m, in \u001b[0;36mLlamafile._batch_generate\u001b[1;34m(self, prompts, use_tqdm, **kwargs)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_batch_generate\u001b[39m(\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], use_tqdm: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 527\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_server_running\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompt) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py:545\u001b[0m, in \u001b[0;36mLlamafile._ensure_server_running\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_server_running\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_running:\n\u001b[1;32m--> 545\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_llamafile_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_server_running():\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py:369\u001b[0m, in \u001b[0;36mLlamafile.start_llamafile_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting llamafile server with command: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommand\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllamafile_process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_llamafile_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_server_start()\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\flow_judge\\models\\llamafile.py:422\u001b[0m, in \u001b[0;36mLlamafile._start_llamafile_process\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(pipe\u001b[38;5;241m.\u001b[39mreadline, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    420\u001b[0m         log_func(line\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m--> 422\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshlex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43muniversal_newlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# preexec_fn=os.setsid,\u001b[39;49;00m\n\u001b[0;32m    430\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubprocess started with PID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocess\u001b[38;5;241m.\u001b[39mpid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# Register cleanup function for this specific process\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\subprocess.py:1024\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m   1021\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1022\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\subprocess.py:1493\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1493\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1495\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1506\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1509\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1510\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。"
     ]
    }
   ],
   "source": [
    "for model_name in models_map.keys():\n",
    "    print(model_name + \": feedbacks\")\n",
    "    saveJudge(model_name, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in models_map.keys():\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "    print(model_name + \":\")\n",
    "    for i in range(len(evaluation_dataset)):\n",
    "        print(evaluation_dataset[i][\"score\"], evaluation_dataset[i][\"generate time\"], evaluation_dataset[i][\"feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
