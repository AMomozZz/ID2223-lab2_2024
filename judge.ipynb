{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lab/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fibonnaci sekvansi ile 1, 1, 2, 3, 5, 8,...continuation\n",
      "Fibonnaci serisi\n",
      "1 1 2 3 5 8 13 21 34 55...\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EITD/orpo_llama\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EITD/orpo_llama\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "if \"assistant\" in response:\n",
    "    response1 = response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To continue the Fibonacci sequence, the next numbers would be:\n",
      "\n",
      "11, 18, 29, 47, 76, 123, 199, 317, 514, 832, 1328,...\n",
      "\n",
      "This is the Fibonacci sequence with the first six numbers given: 1, 1,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "if \"assistant\" in response:\n",
    "    response2 = response.split(\"assistant\")[-1].strip()\n",
    "\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting llamafile server...\n",
      "INFO:root:Llamafile path: /home/jovyan/.cache/flow-judge/flow-judge.llamafile\n",
      "INFO:root:Quantized KV enabled\n",
      "INFO:root:Flash Attention enabled\n",
      "INFO:root:Starting llamafile server with command: sh -c '/home/jovyan/.cache/flow-judge/flow-judge.llamafile --server --host 127.0.0.1 --port 8086 -c 8192 -ngl 34 --temp 0.1 -n 1000 --threads 224 --nobrowser -b 32 --parallel 1 --cont-batching -ctk q4_0 -ctv q4_0 -fa'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Subprocess started with PID: 468199\n",
      "INFO:openai._base_client:Retrying request to /models in 0.402780 seconds\n",
      "INFO:root:import_cuda_impl: initializing gpu module...\n",
      "INFO:root:get_rocm_bin_path: note: hipcc not found on $PATH\n",
      "INFO:root:get_rocm_bin_path: note: $HIP_PATH/bin/hipcc does not exist\n",
      "INFO:root:get_rocm_bin_path: note: /opt/rocm/bin/hipcc does not exist\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/jovyan/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:root:link_cuda_dso: warning: libamdhip64.so.6: cannot open shared object file: No such file or directory: failed to load library\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/jovyan/.llamafile/v/0.8.13/ggml-rocm.so\n",
      "INFO:root:link_cuda_dso: warning: libamdhip64.so.6: cannot open shared object file: No such file or directory: failed to load library\n",
      "INFO:root:get_nvcc_path: note: nvcc not found on $PATH\n",
      "INFO:root:get_nvcc_path: note: $CUDA_PATH/bin/nvcc does not exist\n",
      "INFO:root:get_nvcc_path: note: /opt/cuda/bin/nvcc does not exist\n",
      "INFO:root:get_nvcc_path: note: /usr/local/cuda/bin/nvcc does not exist\n",
      "INFO:root:link_cuda_dso: note: dynamically linking /home/jovyan/.llamafile/v/0.8.13/ggml-cuda.so\n",
      "INFO:root:ggml_cuda_link: welcome to CUDA SDK with tinyBLAS\n",
      "INFO:root:link_cuda_dso: GPU support loaded\n",
      "INFO:root:{\"build\":1500,\"commit\":\"a30b324\",\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2841,\"msg\":\"build info\",\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:root:{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":2844,\"msg\":\"system info\",\"n_threads\":224,\"n_threads_batch\":-1,\"system_info\":\"AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \",\"tid\":\"11681088\",\"timestamp\":1733176101,\"total_threads\":224}\n",
      "INFO:root:llama_model_loader: loaded meta data with 33 key-value pairs and 197 tensors from flow-judge-v0.1-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "INFO:root:llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "INFO:root:llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "INFO:root:llama_model_loader: - kv   1:                               general.type str              = model\n",
      "INFO:root:llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
      "INFO:root:llama_model_loader: - kv   3:                       general.organization str              = Microsoft\n",
      "INFO:root:llama_model_loader: - kv   4:                           general.finetune str              = instruct\n",
      "INFO:root:llama_model_loader: - kv   5:                           general.basename str              = Phi-3.5\n",
      "INFO:root:llama_model_loader: - kv   6:                         general.size_label str              = mini\n",
      "INFO:root:llama_model_loader: - kv   7:                        phi3.context_length u32              = 131072\n",
      "INFO:root:llama_model_loader: - kv   8:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "INFO:root:llama_model_loader: - kv   9:                      phi3.embedding_length u32              = 3072\n",
      "INFO:root:llama_model_loader: - kv  10:                   phi3.feed_forward_length u32              = 8192\n",
      "INFO:root:llama_model_loader: - kv  11:                           phi3.block_count u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  12:                  phi3.attention.head_count u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  13:               phi3.attention.head_count_kv u32              = 32\n",
      "INFO:root:llama_model_loader: - kv  14:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "INFO:root:llama_model_loader: - kv  15:                  phi3.rope.dimension_count u32              = 96\n",
      "INFO:root:llama_model_loader: - kv  16:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "INFO:root:llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "INFO:root:llama_model_loader: - kv  18:              phi3.attention.sliding_window u32              = 262144\n",
      "INFO:root:llama_model_loader: - kv  19:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "INFO:root:llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = llama\n",
      "INFO:root:llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = default\n",
      "INFO:root:llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "INFO:root:llama_model_loader: - kv  23:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "INFO:root:llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "INFO:root:llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "INFO:root:llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "INFO:root:llama_model_loader: - kv  27:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "INFO:root:llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "INFO:root:llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = false\n",
      "INFO:root:llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
      "INFO:root:llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "INFO:root:llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "INFO:root:llama_model_loader: - type  f32:   67 tensors\n",
      "INFO:root:llama_model_loader: - type q4_K:   81 tensors\n",
      "INFO:root:llama_model_loader: - type q5_K:   32 tensors\n",
      "INFO:root:llama_model_loader: - type q6_K:   17 tensors\n",
      "INFO:root:llm_load_vocab: special tokens cache size = 14\n",
      "INFO:root:llm_load_vocab: token to piece cache size = 0.1685 MB\n",
      "INFO:root:llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "INFO:root:llm_load_print_meta: arch             = phi3\n",
      "INFO:root:llm_load_print_meta: vocab type       = SPM\n",
      "INFO:root:llm_load_print_meta: n_vocab          = 32064\n",
      "INFO:root:llm_load_print_meta: n_merges         = 0\n",
      "INFO:root:llm_load_print_meta: vocab_only       = 0\n",
      "INFO:root:llm_load_print_meta: n_ctx_train      = 131072\n",
      "INFO:root:llm_load_print_meta: n_embd           = 3072\n",
      "INFO:root:llm_load_print_meta: n_layer          = 32\n",
      "INFO:root:llm_load_print_meta: n_head           = 32\n",
      "INFO:root:llm_load_print_meta: n_head_kv        = 32\n",
      "INFO:root:llm_load_print_meta: n_rot            = 96\n",
      "INFO:root:llm_load_print_meta: n_swa            = 262144\n",
      "INFO:root:llm_load_print_meta: n_embd_head_k    = 96\n",
      "INFO:root:llm_load_print_meta: n_embd_head_v    = 96\n",
      "INFO:root:llm_load_print_meta: n_gqa            = 1\n",
      "INFO:root:llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "INFO:root:llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "INFO:root:llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "INFO:root:llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "INFO:root:llm_load_print_meta: n_ff             = 8192\n",
      "INFO:root:llm_load_print_meta: n_expert         = 0\n",
      "INFO:root:llm_load_print_meta: n_expert_used    = 0\n",
      "INFO:root:llm_load_print_meta: causal attn      = 1\n",
      "INFO:root:llm_load_print_meta: pooling type     = 0\n",
      "INFO:root:llm_load_print_meta: rope type        = 2\n",
      "INFO:root:llm_load_print_meta: rope scaling     = linear\n",
      "INFO:root:llm_load_print_meta: freq_base_train  = 10000.0\n",
      "INFO:root:llm_load_print_meta: freq_scale_train = 1\n",
      "INFO:root:llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "INFO:root:llm_load_print_meta: rope_finetuned   = unknown\n",
      "INFO:root:llm_load_print_meta: ssm_d_conv       = 0\n",
      "INFO:root:llm_load_print_meta: ssm_d_inner      = 0\n",
      "INFO:root:llm_load_print_meta: ssm_d_state      = 0\n",
      "INFO:root:llm_load_print_meta: ssm_dt_rank      = 0\n",
      "INFO:root:llm_load_print_meta: model type       = 3B\n",
      "INFO:root:llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "INFO:root:llm_load_print_meta: model params     = 3.82 B\n",
      "INFO:root:llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW)\n",
      "INFO:root:llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
      "INFO:root:llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "INFO:root:llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "INFO:root:llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "INFO:root:llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "INFO:root:llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "INFO:root:llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "INFO:root:llm_load_print_meta: max token length = 48\n",
      "INFO:root:ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "INFO:root:ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "INFO:root:ggml_cuda_init: found 1 CUDA devices:\n",
      "INFO:root:Device 0: NVIDIA H100 80GB HBM3 MIG 8g.80gb, compute capability 9.0, VMM: yes\n",
      "INFO:root:llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "INFO:root:llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "INFO:root:llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "INFO:root:llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "INFO:root:llm_load_tensors:      CUDA0 buffer size =  2151.77 MiB\n",
      "INFO:openai._base_client:Retrying request to /models in 0.771774 seconds\n",
      "INFO:root:............................................................................................\n",
      "INFO:root:llama_new_context_with_model: n_ctx      = 8192\n",
      "INFO:root:llama_new_context_with_model: n_batch    = 32\n",
      "INFO:root:llama_new_context_with_model: n_ubatch   = 32\n",
      "INFO:root:llama_new_context_with_model: flash_attn = 1\n",
      "INFO:root:llama_new_context_with_model: freq_base  = 10000.0\n",
      "INFO:root:llama_new_context_with_model: freq_scale = 1\n",
      "INFO:root:llama_kv_cache_init:      CUDA0 KV buffer size =   864.00 MiB\n",
      "INFO:root:llama_new_context_with_model: KV self size  =  864.00 MiB, K (q4_0):  432.00 MiB, V (q4_0):  432.00 MiB\n",
      "INFO:root:llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "INFO:root:llama_new_context_with_model:      CUDA0 compute buffer size =    81.35 MiB\n",
      "INFO:root:llama_new_context_with_model:  CUDA_Host compute buffer size =    28.38 MiB\n",
      "INFO:root:llama_new_context_with_model: graph nodes  = 1159\n",
      "INFO:root:llama_new_context_with_model: graph splits = 68\n",
      "INFO:root:{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":491,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:root:{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":500,\"msg\":\"new slot\",\"n_ctx_slot\":8192,\"slot_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:root:\n",
      "INFO:root:{\"function\":\"server_cli\",\"level\":\"INFO\",\"line\":3062,\"msg\":\"model loaded\",\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:root:llama server listening at http://127.0.0.1:8086\n",
      "INFO:root:{\"function\":\"server_cli\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3185,\"msg\":\"HTTP server listening\",\"port\":\"8086\",\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:root:\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1661,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"11681088\",\"timestamp\":1733176101}\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8086/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/models\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60712,\"status\":200,\"tid\":\"140169034802400\",\"timestamp\":1733176102}\n",
      "INFO:root:Llamafile server started successfully\n",
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:8086/v1/models \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/models\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60712,\"status\":200,\"tid\":\"140169034802400\",\"timestamp\":1733176102}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176102}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176102}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =    1929.44 ms /   896 tokens (    2.15 ms per token,   464.38 tokens per second)\",\"n_tokens_second\":464.3832073642055,\"num_prompt_tokens_processed\":896,\"slot_id\":0,\"t_prompt_processing\":1929.441,\"t_token\":2.153393973214286,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176114}\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8086/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =   10255.22 ms /   273 runs   (   37.56 ms per token,    26.62 tokens per second)\",\"n_decoded\":273,\"n_tokens_second\":26.620586723582065,\"slot_id\":0,\"t_token\":37.564912087912084,\"t_token_generation\":10255.221,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176114}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =   12184.66 ms\",\"slot_id\":0,\"t_prompt_processing\":1929.441,\"t_token_generation\":10255.221,\"t_total\":12184.662,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176114}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1169,\"n_ctx\":8192,\"n_past\":1168,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":0,\"tid\":\"11681088\",\"timestamp\":1733176114,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60712,\"status\":200,\"tid\":\"140169034802400\",\"timestamp\":1733176114}\n",
      "INFO:root:{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":886,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176114}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1912,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176114}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response is completely consistent with and faithful to the provided context. The task was to continue the Fibonacci sequence, and the output correctly does so by listing the next numbers in the sequence (13, 21, 34, 55) after the given part (1, 1, 2, 3, 5, 8). There is no hallucinated or fabricated information in the response. The output is directly supported by the context, which only required continuing the sequence, and the AI system has done so accurately.\n",
      "\n",
      "The response demonstrates a clear understanding of the Fibonacci sequence's rule (each number is the sum of the two preceding ones) and applies it correctly to generate the subsequent numbers. There is no deviation from the context or introduction of any unsupported information. The output is precise, concise, and entirely focused on the task at hand, demonstrating a high level of consistency with the provided input.\n",
      "\n",
      "Overall, the response perfectly meets the requirements of the task and maintains complete fidelity to the given context, making it a flawless execution of the requested continuation of the Fibonacci sequence. 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":313,\"msg\":\"prompt eval time     =    2000.00 ms /   924 tokens (    2.16 ms per token,   462.00 tokens per second)\",\"n_tokens_second\":461.99907600184804,\"num_prompt_tokens_processed\":924,\"slot_id\":0,\"t_prompt_processing\":2000.004,\"t_token\":2.1645064935064933,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176131}\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:8086/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":327,\"msg\":\"generation eval time =   14844.98 ms /   397 runs   (   37.39 ms per token,    26.74 tokens per second)\",\"n_decoded\":397,\"n_tokens_second\":26.743052549020454,\"slot_id\":0,\"t_token\":37.39288916876575,\"t_token_generation\":14844.977,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176131}\n",
      "INFO:root:{\"function\":\"print_timings\",\"level\":\"INFO\",\"line\":337,\"msg\":\"          total time =   16844.98 ms\",\"slot_id\":0,\"t_prompt_processing\":2000.004,\"t_token_generation\":14844.977,\"t_total\":16844.981,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176131}\n",
      "INFO:root:{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1723,\"msg\":\"slot released\",\"n_cache_tokens\":1321,\"n_ctx\":8192,\"n_past\":1320,\"n_system_tokens\":0,\"slot_id\":0,\"task_id\":275,\"tid\":\"11681088\",\"timestamp\":1733176131,\"truncated\":false}\n",
      "INFO:root:{\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2766,\"method\":\"POST\",\"msg\":\"request\",\"params\":{},\"path\":\"/v1/chat/completions\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":60712,\"status\":200,\"tid\":\"140169034802400\",\"timestamp\":1733176131}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response provided is completely inconsistent with the given context and does not follow the Fibonacci sequence pattern. The Fibonacci sequence is a series of numbers where each number is the sum of the two preceding ones. The given context starts with \"1, 1, 2, 3, 5, 8,\" which clearly indicates the beginning of a Fibonacci sequence. However, the output provided does not continue this pattern. Instead, it lists numbers that do not follow the Fibonacci rule (e.g., 11, 18, 29, 47, etc.), which are not the sum of the two preceding numbers. Therefore, the response contains significant amounts of hallucinated or fabricated information that is not supported by the given context.\n",
      "\n",
      "The output does not align with the Fibonacci sequence pattern and introduces numbers that are not part of the sequence derived from the given context. For instance, the sequence \"1, 1, 2, 3, 5, 8\" should have continued with \"13, 21, 34, 55, 89,\" but the output shows \"11, 18, 29, 47, 76,\" which are incorrect and do not match the Fibonacci rule.\n",
      "\n",
      "Given that the context is to continue the Fibonacci sequence, the response fails to meet the task requirements and introduces fabricated information that deviates from the expected pattern. This makes the response entirely inconsistent with the provided context.\n",
      "\n",
      "Based on the evaluation criteria and scoring rubric, the response should be scored as a 1 because it is completely inconsistent with the provided context and contains significant amounts of fabricated information. 1\n"
     ]
    }
   ],
   "source": [
    "from flow_judge import Vllm, Llamafile, Hf, EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from flow_judge.metrics import list_all_metrics\n",
    "\n",
    "# list_all_metrics()\n",
    "\n",
    "# Or create a model using Llamafile if not running an Nvidia GPU & running a Silicon MacOS for example\n",
    "judge = Llamafile()\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=judge\n",
    ")\n",
    "\n",
    "# Sample to evaluate\n",
    "query = messages[0]['content']\n",
    "context = \"\"\n",
    "\n",
    "# Create an EvalInput\n",
    "# We want to evaluate the response to the customer issue based on the context and the user instructions\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response1},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)\n",
    "\n",
    "# Display the result\n",
    "print(result.feedback, result.score)\n",
    "\n",
    "eval_input = EvalInput(\n",
    "    inputs=[\n",
    "        {\"query\": query},\n",
    "        {\"context\": context},\n",
    "    ],\n",
    "    output={\"response\": response2},\n",
    ")\n",
    "\n",
    "# Run the evaluation\n",
    "result = faithfulness_judge.evaluate(eval_input, save_results=False)\n",
    "\n",
    "# Display the result\n",
    "print(result.feedback, result.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
