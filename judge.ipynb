{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/envs/lab/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: flow_judge in /opt/conda/envs/lab/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->-r requirements.txt (line 1)) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (3.11.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (0.26.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge->-r requirements.txt (line 2)) (2.10.3)\n",
      "Requirement already satisfied: hf-transfer>=0.1.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge->-r requirements.txt (line 2)) (0.1.8)\n",
      "Requirement already satisfied: ipykernel>=6.29.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge->-r requirements.txt (line 2)) (6.29.5)\n",
      "Requirement already satisfied: ipywidgets>=8.1.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge->-r requirements.txt (line 2)) (8.1.5)\n",
      "Requirement already satisfied: structlog in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge->-r requirements.txt (line 2)) (24.4.0)\n",
      "Requirement already satisfied: vllm==0.6.2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from flow_judge[vllm]->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.45.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (4.46.3)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.20.3)\n",
      "Requirement already satisfied: protobuf in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.20.3)\n",
      "Requirement already satisfied: openai>=1.40.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.57.0)\n",
      "Requirement already satisfied: uvicorn[standard] in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.32.1)\n",
      "Requirement already satisfied: pillow in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (10.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.21.1)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.10.6 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.10.6)\n",
      "Requirement already satisfied: outlines<0.1,>=0.0.43 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: partial-json-parser in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.2.1.1.post4)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (26.2.0)\n",
      "Requirement already satisfied: msgspec in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.18.6)\n",
      "Requirement already satisfied: gguf==0.10.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (8.5.0)\n",
      "Requirement already satisfied: mistral-common>=1.4.3 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: einops in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: ray>=2.9 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2.40.0)\n",
      "Requirement already satisfied: nvidia-ml-py in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.560.30)\n",
      "Requirement already satisfied: torch==2.4.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: torchvision==0.19 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.19.0)\n",
      "Requirement already satisfied: xformers==0.0.27.post2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.0.27.post2)\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: setuptools>=74.1.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (75.6.0)\n",
      "Requirement already satisfied: fastapi>=0.114.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.115.6)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from lm-format-enforcer==0.10.6->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.3.3)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/envs/lab/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 1)) (1.18.3)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (1.8.9)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (8.30.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipywidgets>=8.1.0->flow_judge->-r requirements.txt (line 2)) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipywidgets>=8.1.0->flow_judge->-r requirements.txt (line 2)) (3.0.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pydantic>=2.9.1->flow_judge->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pydantic>=2.9.1->flow_judge->-r requirements.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/lab/lib/python3.12/site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 1)) (2024.2)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from fastapi>=0.114.1->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.41.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: stack_data in /opt/conda/envs/lab/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (4.3.6)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.21.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from mistral-common>=1.4.3->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (4.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.28.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/lab/lib/python3.12/site-packages (from openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: lark in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: diskcache in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (5.6.3)\n",
      "Requirement already satisfied: numba in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.60.0)\n",
      "Requirement already satisfied: referencing in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.35.1)\n",
      "Requirement already satisfied: pycountry in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (24.6.1)\n",
      "Requirement already satisfied: pyairports in /opt/conda/envs/lab/lib/python3.12/site-packages (from outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ray>=2.9->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from ray>=2.9->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/envs/lab/lib/python3.12/site-packages (from tiktoken>=0.6.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from transformers>=4.45.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.4.5)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/conda/envs/lab/lib/python3.12/site-packages (from importlib-metadata->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.21.0)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/envs/lab/lib/python3.12/site-packages (from uvicorn[standard]->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (14.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/lab/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.40.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/envs/lab/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.8.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/lab/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.3->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/lab/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.21.1->mistral-common>=1.4.3->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.22.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/lab/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/lab/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.2.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from jinja2->torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (0.43.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/envs/lab/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel>=6.29.0->flow_judge->-r requirements.txt (line 2)) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/lab/lib/python3.12/site-packages (from sympy->torch==2.4.0->vllm==0.6.2->flow_judge[vllm]->-r requirements.txt (line 3)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Create a solution in Kotlin for the following task: Associative array/Merging\\n\\nDefine two associative arrays, where one represents the following \"base\" data:\\n\\nKey - Value\\n\"name\" - \"Rocket Skates\"\\n\"price\" - 12.75\\n\"color\" - \"yellow\"\\n\\nAnd the other represents \"update\" data:\\n\\nKey - Value\\n\"price\" - 15.25\\n\"color\" - \"red\"\\n\"year\" - 1974\\n\\nMerge these into a new associative array that contains every key found in either of the source ones. Each key should map to the value in the second (update) table if that exists, or else to the value in the first (base) table. If possible, do this in a way that does not mutate the original two associative arrays. Obviously this should be done in a way that would work for any data, not just the specific data given here, but in this example the result should be:\\n\\nKey - Value\\n\"name\" - \"Rocket Skates\"\\n\"price\" - 15.25\\n\"color\" - \"red\"\\n\"year\" - 1974 PLAINFORMAT\\n'}, {'role': 'assistant', 'content': 'fun main() {\\n    val base = HashMap<String,String>()\\n    val update =  HashMap<String,String>()\\n\\n    base[\"name\"] = \"Rocket Skates\"\\n    base[\"price\"] = \"12.75\"\\n    base[\"color\"] = \"yellow\"\\n\\n    update[\"price\"] = \"15.25\"\\n    update[\"color\"] = \"red\"\\n    update[\"year\"] = \"1974\"\\n\\n    val merged = HashMap(base)\\n    merged.putAll(update)\\n\\n    println(\"base: $base\")\\n    println(\"update: $update\")\\n    println(\"merged: $merged\")\\n}\\n'}, {'role': 'user', 'content': \"Considering the associative array merging solution in Kotlin, how would you modify the code to handle null values in the update associative array, ensuring they don't overwrite existing valid values in the base associative array?\"}, {'role': 'assistant', 'content': 'You can modify the code to check for null values before adding them to the merged associative array. Here\\'s how you can do it:\\n\\n```kotlin\\nfun main() {\\n    val base = HashMap<String, Any?>()\\n    val update =  HashMap<String, Any?>()\\n\\n    base[\"name\"] = \"Rocket Skates\"\\n    base[\"price\"] = 12.75\\n    base[\"color\"] = \"yellow\"\\n\\n    update[\"price\"] = 15.25\\n    update[\"color\"] = null\\n    update[\"year\"] = 1974\\n\\n    val merged = HashMap(base)\\n    update.forEach { (key, value) ->\\n        if (value != null) {\\n            merged[key] = value\\n        }\\n    }\\n\\n    println(\"base: $base\")\\n    println(\"update: $update\")\\n    println(\"merged: $merged\")\\n}\\n```\\n\\nIn this code, we iterate over each entry in the update associative array. If the value is not null, we add it to the merged associative array. This way, null values in the update associative array will not overwrite existing valid values in the base associative array.'}, {'role': 'user', 'content': 'What would be the potential impact on performance if the base and update associative arrays were significantly larger, and how could this be mitigated?'}]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "\n",
    "dataset_name = \"LDJnr/Capybara\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\").shuffle(seed=42)\n",
    "\n",
    "messages = []\n",
    "for row in list(dataset):\n",
    "    message = []\n",
    "    for i, item in enumerate(row[\"conversation\"]):\n",
    "        user = {\"role\": \"user\"}\n",
    "        user[\"content\"] = item[\"input\"]\n",
    "        message.append(user)\n",
    "        if i < len(row[\"conversation\"]) - 1:\n",
    "            assistant = {\"role\": \"assistant\"}\n",
    "            assistant[\"content\"] = item[\"output\"]\n",
    "            message.append(assistant)\n",
    "    messages.append(message)\n",
    "\n",
    "messages = random.sample(messages, 200)\n",
    "print(messages[0])\n",
    "\n",
    "orpo_path = \"orpo.json\"\n",
    "llama_path = \"llama.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lab/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 8g.80gb. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.2 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import json, time\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"EITD/orpo\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "responses = []\n",
    "for message in messages:\n",
    "    start = time.time()        \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                            temperature = 1.5, min_p = 0.1)\n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    if \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    end = time.time()         \n",
    "\n",
    "    responses.append({'time': end - start, 'response': response})\n",
    "\n",
    "with open(orpo_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.2: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3 MIG 8g.80gb. Max memory: 79.109 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.0+cu121. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import json, time\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "responses = []\n",
    "for message in messages:\n",
    "    start = time.time()\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        message,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True, temperature = 1.5, min_p = 0.1)\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    if \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    end = time.time()         \n",
    "\n",
    "    responses.append({'time': end - start, 'response': response})\n",
    "\n",
    "with open(llama_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/flowaicom/Flow-Judge-v0.1-AWQ:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 18:36:23 awq_marlin.py:90] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "WARNING 12-06 18:36:23 config.py:389] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 12-06 18:36:23 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='flowaicom/Flow-Judge-v0.1-AWQ', speculative_config=None, tokenizer='flowaicom/Flow-Judge-v0.1-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=flowaicom/Flow-Judge-v0.1-AWQ, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 12-06 18:36:23 model_runner.py:1014] Starting to load model flowaicom/Flow-Judge-v0.1-AWQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1206 18:36:23.435164708 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 18:36:24 weight_utils.py:242] Using model weights format ['*.safetensors']\n",
      "INFO 12-06 18:36:24 weight_utils.py:287] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abae16452d4d453c952c53da6164e9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-06 18:36:30 model_runner.py:1025] Loading model weights took 2.1700 GB\n",
      "INFO 12-06 18:36:30 gpu_executor.py:122] # GPU blocks: 11669, # CPU blocks: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 12-06 18:36:39 scheduler.py:1439] Sequence group 182 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:28<00:00,  7.01it/s, est. speed input: 6995.11 toks/s, output: 1647.95 toks/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:28<00:00,  7.02it/s, est. speed input: 7033.27 toks/s, output: 1659.79 toks/s]\n"
     ]
    }
   ],
   "source": [
    "from flow_judge import Llamafile, EvalInput, FlowJudge, Vllm\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "import json\n",
    "\n",
    "# Initialize the model\n",
    "judge = Vllm()\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=judge\n",
    ")\n",
    "\n",
    "# Create a list of inputs and outputs\n",
    "inputs_batch = [\n",
    "    [\n",
    "        {\"query\": message[-1][\"content\"]},\n",
    "        {\"context\": \"\"},\n",
    "    ]\n",
    "    for message in messages\n",
    "]\n",
    "\n",
    "def judge(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    outputs_batch = [{\"response\": item[\"response\"]} for item in data]\n",
    "\n",
    "    # Create a list of EvalInput\n",
    "    eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "    # Run the batch evaluation\n",
    "    results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        data[i]['score'] = result.score\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "judge(orpo_path)\n",
    "judge(llama_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orpo metrics:\n",
      "Avg Inference Time: 1.6124457895755768\n",
      "Avg Score: 3.51\n",
      "Llama metrics:\n",
      "Avg Inference Time: 1.3945598220825195\n",
      "Avg Score: 3.395\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    time_sum = 0\n",
    "    score_sum = 0\n",
    "    for item in data:\n",
    "        time_sum += item['time']\n",
    "        score_sum += item['score']\n",
    "\n",
    "    print(\"Avg Inference Time:\", time_sum / len(data))\n",
    "    print(\"Avg Score:\", score_sum / len(data))\n",
    "\n",
    "print(\"Orpo metrics:\")\n",
    "compute_metrics(orpo_path)\n",
    "print(\"Llama metrics:\")\n",
    "compute_metrics(llama_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
