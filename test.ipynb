{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"AMomozZz/model\",\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at AMomozZz/model and are newly initialized: ['lm_head.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at AMomozZz/model and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([128256, 3072]) in the checkpoint and torch.Size([32000, 4096]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.0.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.0.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.0.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.1.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.1.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.1.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.1.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.1.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.1.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.1.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.10.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.10.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.10.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.10.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.10.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.10.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.10.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.11.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.11.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.11.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.11.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.11.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.11.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.11.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.12.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.12.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.12.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.12.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.12.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.12.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.12.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.13.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.13.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.13.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.13.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.13.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.13.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.13.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.14.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.14.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.14.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.14.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.14.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.14.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.14.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.15.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.15.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.15.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.15.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.15.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.16.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.16.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.16.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.16.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.16.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.17.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.17.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.17.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.17.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.17.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.18.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.18.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.18.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.18.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.18.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.19.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.19.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.19.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.19.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.19.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.19.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.19.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.19.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.19.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.2.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.2.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.2.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.2.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.2.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.2.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.2.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.20.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.20.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.20.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.20.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.20.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.20.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.20.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.20.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.20.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.21.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.21.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.21.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.21.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.21.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.21.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.21.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.21.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.21.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.22.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.22.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.22.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.22.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.22.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.22.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.22.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.22.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.22.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.23.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.23.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.23.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.23.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.23.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.23.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.23.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.23.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.23.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.24.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.24.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.24.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.24.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.24.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.24.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.24.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.24.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.24.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.25.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.25.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.25.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.25.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.25.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.25.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.25.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.25.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.25.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.26.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.26.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.26.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.26.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.26.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.26.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.26.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.26.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.26.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.27.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.27.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.27.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.27.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.27.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.27.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.27.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.27.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.27.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.3.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.3.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.3.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.3.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.3.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.3.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.3.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.4.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.4.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.4.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.4.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.4.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.4.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.4.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.5.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.5.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.5.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.5.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.5.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.5.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.5.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.6.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.6.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.6.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.6.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.6.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.6.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.6.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.7.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.7.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.7.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.7.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.7.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.7.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.7.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.8.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.8.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.8.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.8.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.8.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.8.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.8.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.9.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.9.mlp.down_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([4096, 11008]) in the model instantiated\n",
      "- model.layers.9.mlp.gate_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.9.mlp.up_proj.weight: found shape torch.Size([12582912, 1]) in the checkpoint and torch.Size([11008, 4096]) in the model instantiated\n",
      "- model.layers.9.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.9.self_attn.o_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.9.self_attn.q_proj.weight: found shape torch.Size([4718592, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([1572864, 1]) in the checkpoint and torch.Size([4096, 4096]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([4096]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d949a447ca449b9ba5d86012117d2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AMomozZz/model\")\n",
    "config = AutoConfig.from_pretrained(\"AMomozZz/model\")\n",
    "print(config)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AMomozZz/model\", ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 模型前向传播\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 获取最后一个隐藏层的输出\u001b[39;00m\n\u001b[0;32m     11\u001b[0m last_hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:891\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    888\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 891\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n\u001b[0;32m    894\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3112\\Lib\\site-packages\\torch\\nn\\functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# 输入文本\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# 编码输入\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 模型前向传播\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# 获取最后一个隐藏层的输出\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "# print(last_hidden_states.shape)  # [batch_size, seq_length, hidden_size]\n",
    "print(last_hidden_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
