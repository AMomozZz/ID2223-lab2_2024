{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import time\n",
    "\n",
    "tokenizer_orpo_llama = AutoTokenizer.from_pretrained(\"EITD/orpo_llama\")\n",
    "model_orpo_llama = AutoModelForCausalLM.from_pretrained(\"EITD/orpo_llama\")\n",
    "\n",
    "dataset_name = \"mlabonne/FineTome-100k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\").shuffle(seed=42)\n",
    "\n",
    "# messages = [{\"role\": \"user\", \"content\": prompt} for prompt in dataset[\"test\"][\"prompt\"]]\n",
    "# messages = [{\"role\": \"user\", \"content\": feature['conversations'][0]['value']} for feature in dataset[\"test\"]]\n",
    "messages = dataset.map(\n",
    "    lambda row: {\"role\": \"user\", \"content\": next((item[\"value\"] for item in row[\"conversations\"] if item[\"from\"] == \"human\"), None)},\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "messages = messages.select(range(3))\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Say some thing about Olympics\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Give me some advice about making cookies\"},\n",
    "# ]\n",
    "\n",
    "# messages = pd.DataFrame(messages)\n",
    "\n",
    "evaluation_matrix_orpo_llama = Dataset.from_dict({\"index\": [],\"response\": [], \"feedback\": [],\"generate time\": [],\"score\": []})\n",
    "# responses_orpo_llama = pd.DataFrame(columns=[\"response\", \"feedback\", \"generate time\", \"score\"])\n",
    "evaluation_datasets = {\"orpo_llama\": evaluation_matrix_orpo_llama}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['role', 'content'],\n",
       "     num_rows: 3\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['index', 'response', 'feedback', 'generate time', 'score'],\n",
       "     num_rows: 0\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages, evaluation_matrix_orpo_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResponse(model_name, messages):\n",
    "    tokenizer = tokenizer_orpo_llama\n",
    "    model = model_orpo_llama\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "\n",
    "    for i, message in enumerate(messages):\n",
    "        \n",
    "        start = time.time()         # start time\n",
    "        # inputs = tokenizer.apply_chat_template(\n",
    "        #     [message],\n",
    "        #     tokenize = True,\n",
    "        #     add_generation_prompt = True, # Must add for generation\n",
    "        #     return_tensors = \"pt\",\n",
    "        # )\n",
    "        \n",
    "        # outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "        #                         temperature = 1.5, min_p = 0.1)\n",
    "        \n",
    "        # response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        end = time.time()           # end time\n",
    "\n",
    "        # if \"assistant\" in response:\n",
    "        #     response = response.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        response = \"right answer\"\n",
    "        \n",
    "        evaluation_dataset = evaluation_dataset.add_item({\"index\": i, \"response\": response, \"feedback\": None, \"generate time\": end-start, \"score\": None})\n",
    "    \n",
    "    evaluation_datasets[model_name] = evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveResponse(\"orpo_llama\", messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample data\n",
    "from flow_judge import Vllm, Llamafile, Hf, EvalInput, FlowJudge\n",
    "from flow_judge.metrics import RESPONSE_FAITHFULNESS_5POINT\n",
    "from flow_judge.metrics import list_all_metrics\n",
    "\n",
    "# Initialize the model\n",
    "judge = Llamafile()\n",
    "\n",
    "# Initialize the judge\n",
    "faithfulness_judge = FlowJudge(\n",
    "    metric=RESPONSE_FAITHFULNESS_5POINT,\n",
    "    model=judge\n",
    ")\n",
    "\n",
    "def add_feedback_and_score(example, result):\n",
    "    return {\n",
    "        \"response\": example['response'],\n",
    "        \"generate time\": example['generate time'],\n",
    "        \"feedback\": result.feedback,\n",
    "        \"score\": result.score\n",
    "    }\n",
    "\n",
    "def saveJudge(model_name, messages):\n",
    "    evaluation_dataset = evaluation_datasets.get(model_name)\n",
    "\n",
    "    # Create a list of inputs and outputs\n",
    "    inputs_batch = [\n",
    "        [\n",
    "            {\"query\": message[\"content\"]},\n",
    "            {\"context\": \"\"},\n",
    "        ]\n",
    "        for message in messages\n",
    "    ]\n",
    "    outputs_batch = [{\"response\": item[\"response\"]} for item in evaluation_dataset]\n",
    "\n",
    "    # Create a list of EvalInput\n",
    "    eval_inputs_batch = [EvalInput(inputs=inputs, output=output) for inputs, output in zip(inputs_batch, outputs_batch)]\n",
    "\n",
    "    # Run the batch evaluation\n",
    "    results = faithfulness_judge.batch_evaluate(eval_inputs_batch, save_results=False)\n",
    "\n",
    "    # Visualizing the results\n",
    "    evaluation_datasets[model_name] = evaluation_dataset.map(lambda item: add_feedback_and_score(item, results[item['index']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'index': 0, 'response': 'right answer', 'feedback': 'some', 'generate time': 0.0, 'score': 2}\n",
      "{'index': 1, 'response': 'right answer', 'feedback': 'other', 'generate time': 0.0, 'score': 1}\n",
      "{'index': 2, 'response': 'right answer', 'feedback': 'things', 'generate time': 0.0, 'score': 5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50aa44bec856481781d8103c4522304f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveJudge(\"orpo_llama\", messages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
